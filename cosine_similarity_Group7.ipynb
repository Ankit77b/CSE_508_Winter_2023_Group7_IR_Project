{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--04mt4awpa-",
        "outputId": "e4518bdf-2478-4b4f-e188-bcdee2691957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch webpage and parse HTML content\n",
        "url = 'https://www.flipkart.com/apple-iphone-13-pink-128-gb/p/itm6e30c6ee045d2'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "BE8hkr3GxfH-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text into sentences\n",
        "sentences = []\n",
        "for paragraph in soup.find_all('p'):\n",
        "    sentences += paragraph.text.split('.')"
      ],
      "metadata": {
        "id": "jxtbcV_bxnXY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_list = [sentence.split() for sentence in sentences]\n",
        "print(words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkriQD9k35J3",
        "outputId": "2262f4f5-63f9-4663-bb70-c87e02bc2ee0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['APPLE', 'iPhone', '13', '(Pink,', '128', 'GB)'], ['iPhone', '13'], ['boasts', 'an', 'advanced', 'dual-camera', 'system', 'that', 'allows', 'you', 'to', 'click', 'mesmerising', 'pictures', 'with', 'immaculate', 'clarity'], ['Furthermore,', 'the', 'lightning-fast', 'A15', 'Bionic', 'chip', 'allows', 'for', 'seamless', 'multitasking,', 'elevating', 'your', 'performance', 'to', 'a', 'new', 'dimension'], ['A', 'big', 'leap', 'in', 'battery', 'life,', 'a', 'durable', 'design,', 'and', 'a', 'bright', 'Super', 'Retina', 'XDR', 'display', 'facilitate', 'boosting', 'your', 'user', 'experience'], [], ['A', 'more', 'vivid', 'OLED', 'display', 'that’s', 'both', 'easier', 'to', 'see', 'in', 'full', 'sunlight', 'and', 'power', 'efficient'], ['With', 'a', 'durable', 'design', 'that’s', 'water', 'and', 'dust-resistant'], [], ['Automatically', 'create', 'beautiful', 'depth', 'effects', 'and', 'focus', 'transitions', 'in', 'your', 'videos'], ['Bring', 'a', 'new', 'level', 'of', 'storytelling', 'to', 'the', 'scenes', 'you', 'shoot'], [], ['A', 'huge', 'upgrade', 'for', 'better', 'photos', 'and', 'videos'], ['More', 'light', 'is', 'captured', 'with', 'the', 'Wide', 'camera'], ['Sensor-shift', 'optical', 'image', 'stabilisation'], ['And', 'more', 'detail', 'in', 'dark', 'areas', 'of', 'your', 'photos', 'with', 'the', 'Ultra', 'Wide', 'camera'], [], ['A', 'big', 'boost', 'in', 'battery', 'life', 'you’ll', 'notice', 'every', 'day'], ['That', 'means', 'more', 'time', 'to', 'watch,', 'game,', 'and', 'do', 'more', 'of', 'what', 'you', 'love', 'with', 'up', 'to', '19', 'hours', 'of', 'video', 'playback', 'on', 'a', 'single', 'charge'], [], ['A15', 'Bionic', 'powers', 'graphics-intensive', 'games', 'and', 'new', 'camera', 'features', 'like', 'Cinematic', 'mode', 'and', 'Photographic', 'Styles'], ['And', 'it’s', 'more', 'efficient,', 'helping', 'deliver', 'longer', 'battery', 'life'], [], ['Perfect', 'product!'], ['Arnab', 'Das'], ['Certified', 'Buyer,', 'Panihati'], ['4', 'months', 'ago'], ['Excellent'], ['Flipkart', 'Customer'], ['Certified', 'Buyer,', 'Bengaluru'], ['4', 'months', 'ago'], ['Brilliant'], ['Mahim', 'Chauhan'], ['Certified', 'Buyer,', 'Vadodara'], ['8', 'months', 'ago'], ['Fabulous!'], ['Vaibhav', 'Raj'], ['Certified', 'Buyer,', 'Rajpura'], ['Oct,', '2021'], ['Fabulous!'], ['sambit', 'sahu'], ['Certified', 'Buyer,', 'Bangalore', 'Urban'], ['7', 'months', 'ago'], ['Fabulous!'], ['Maharshi', 'Pandey'], ['Certified', 'Buyer,', 'Bangalore', 'Urban'], ['5', 'months', 'ago'], ['Simply', 'awesome'], ['Rajan', 'Khan'], ['Certified', 'Buyer,', 'Etawah'], ['5', 'months', 'ago'], ['Must', 'buy!'], ['Vishal', 'Dubey'], ['Certified', 'Buyer,', 'New', 'Delhi'], ['Nov,', '2021'], ['Perfect', 'product!'], ['Flipkart', 'Customer'], ['Certified', 'Buyer,', 'Talcher'], ['Jan,', '2022'], ['Must', 'buy!'], ['Parwej', 'Alam'], ['Certified', 'Buyer,', 'Patna'], ['5', 'months', 'ago'], ['Flipkart', 'Internet', 'Private', 'Limited,'], ['Buildings', 'Alyssa,', 'Begonia', '&'], ['Clove', 'Embassy', 'Tech', 'Village,'], ['Outer', 'Ring', 'Road,', 'Devarabeesanahalli', 'Village,'], ['Bengaluru,', '560103,'], ['Karnataka,', 'India'], ['Flipkart', 'Internet', 'Private', 'Limited,'], ['Buildings', 'Alyssa,', 'Begonia', '&'], ['Clove', 'Embassy', 'Tech', 'Village,'], ['Outer', 'Ring', 'Road,', 'Devarabeesanahalli', 'Village,'], ['Bengaluru,', '560103,'], ['Karnataka,', 'India'], ['CIN', ':', 'U51109KA2012PTC066107'], ['Telephone:', '044-45614700']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the word embedding model\n",
        "model = gensim.models.Word2Vec(words_list, size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJCfV4Krx7ab",
        "outputId": "0f505101-cc6f-4adb-b895-890fdf8ab553"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('my_word2vec_model.bin')"
      ],
      "metadata": {
        "id": "slgjqAi2yBqG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = gensim.models.Word2Vec.load('my_word2vec_model.bin')"
      ],
      "metadata": {
        "id": "vRkzS8vHyLot"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv['display']  # get numpy vector of a word\n",
        "sims = model.wv.most_similar('display', topn=5)  # get other similar words"
      ],
      "metadata": {
        "id": "6K4gSMPwztdS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sims)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdFx-Z4nz07D",
        "outputId": "ba0f9735-1721-4577-82f3-4b454bdf3e02"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('facilitate', 0.29216089844703674), ('of', 0.23448902368545532), ('life,', 0.2045225203037262), ('Bangalore', 0.19726967811584473), ('hours', 0.19189152121543884)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPQpnn2uB3Pm",
        "outputId": "35e5e6e4-a07c-41c9-e417-c119cd8f91ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.1.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.0/648.0 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: annoy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.similarities.index import AnnoyIndexer\n",
        "\n",
        "# 100 trees are being used in this example\n",
        "annoy_index = AnnoyIndexer(model, 100)\n",
        "# Derive the vector for the word \"science\" in our model\n",
        "vector = model.wv[\"display\"]\n",
        "# The instance of AnnoyIndexer we just created is passed\n",
        "approximate_neighbors = model.wv.most_similar([vector], topn=5, indexer=annoy_index)\n",
        "# Neatly print the approximate_neighbors and their corresponding cosine similarity values\n",
        "print(\"Approximate Neighbors\")\n",
        "for neighbor in approximate_neighbors:\n",
        "    print(neighbor)"
      ],
      "metadata": {
        "id": "CACbkrNJBWau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "keyword = 'battery'\n",
        "\n",
        "matching_sentences = []\n",
        "if keyword in model.wv:\n",
        "    keyword_embedding = model.wv[keyword]\n",
        "    for sentence in sentences:\n",
        "            tokens = nltk.word_tokenize(sentence)\n",
        "            # Remove punctuation from the tokens\n",
        "            tokens = [token for token in tokens if token not in string.punctuation]\n",
        "            # Remove special characters and numbers from the tokens\n",
        "            tokens = [token for token in tokens if token.isalpha()]\n",
        "            # Remove stopwords from the tokens\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            tokens = [token for token in tokens if not token in stop_words]\n",
        "            sentence_embedding = sum([model.wv[token] for token in tokens]) / len(tokens)\n",
        "            similarity = model.wv.cosine_similarities(keyword_embedding.reshape(1, -1), sentence_embedding.reshape(1, -1))[0]\n",
        "            if similarity > 0.5:  # Adjust similarity threshold as needed\n",
        "                matching_sentences.append(sentence)\n",
        "                for sentence in matching_sentences:\n",
        "                  print(sentence)\n",
        "else:\n",
        "    print(f\"{keyword} not found in model vocabulary.\")\n"
      ],
      "metadata": {
        "id": "GG2b2VnNmdrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Matching sentences:\")\n",
        "print(matching_sentences)"
      ],
      "metadata": {
        "id": "LJc68he-11is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8F22D6m2f-H_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}